{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52592f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigbasket dynamic products by scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa6b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Connection': 'keep-alive',\n",
    "    'DNT': '1',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d99168-229b-45e6-b96b-861fa88dee45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98b5e9-ab58-44da-90b5-f661166d1b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " beverages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beverages\n",
      "Found 2716 products.\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Edge()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "def search_category():\n",
    "    cat_in = input(\"\").strip().lower()\n",
    "    cat_ab = (\n",
    "        \"apparel\",\"fruits-vegetables\", \"foodgrains-oil-masala\", \"bakery-cakes-dairy\", \"beverages\",\n",
    "        \"snacks-branded-foods\", \"beauty-hygiene\", \"cleaning-household\", \"kitchen-garden-pets\",\n",
    "        \"lunch-boxes-bags\", \"umbrellas-rainwear\",\"eggs-meat-fish\", \"gourmet-world-food\", \"baby-care\", \"paan-corner\"\n",
    "    )\n",
    "    for k in cat_ab:\n",
    "        for j in k.split(\"-\"):\n",
    "            if cat_in in j:\n",
    "                return k\n",
    "    return None\n",
    "\n",
    "def scrap_category(search):\n",
    "    if not search:\n",
    "        return []\n",
    "\n",
    "    url = f\"https://www.bigbasket.com/cl/{search}/?nc=nb\"\n",
    "    driver.get(url)\n",
    "    # time.sleep(2)\n",
    "    product_links = set()  \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True :\n",
    "        driver.execute_script(\"window.scrollBy(0, window.innerHeight*1.4);\")\n",
    "        time.sleep(3)  # Adjust the sleep time if needed\n",
    "        \n",
    "        # Check if the page height has increased\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # Exit if no new content is loaded\n",
    "        last_height = new_height\n",
    "        \n",
    "    # Extract product links from the current view\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    all_cards = soup.find_all('div', class_=\"SKUDeck___StyledDiv-sc-1e5d9gk-0 eA-dmzP\")\n",
    "    for card in all_cards:\n",
    "        link = card.find('a', rel=\"noopener noreferrer\")\n",
    "        if link:\n",
    "            product_links.add(f\"https://www.bigbasket.com{link.get('href')}\")\n",
    "            \n",
    "    return list(product_links)\n",
    "\n",
    "\n",
    "def scrap_product(url_page):\n",
    "    try:\n",
    "        driver.get(url_page)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Extract brand, title, category, price, weight/volume, and images\n",
    "        brand = soup.find('a', class_=\"Description___StyledLink-sc-82a36a-1 gePjxR\")\n",
    "        title = soup.find('h1', class_=\"Description___StyledH-sc-82a36a-2 bofYPK\")\n",
    "        category_elements = soup.find_all('span', class_=\"name text-md leading-md xl:leading-sm xl:text-base text-darkOnyx-800 false\")\n",
    "        price = soup.find('td', class_=\"Description___StyledTd-sc-82a36a-4 fLZywG\")\n",
    "        w_v = soup.find('span', class_=\"Label-sc-15v1nk5-0 PackSizeSelector___StyledLabel2-sc-l9rhbt-2 gJxZPQ hDJUsF\")\n",
    "\n",
    "        # Process extracted elements\n",
    "        brand_text = brand.text.strip() if brand else None\n",
    "        title_text = title.text.strip() if title else None\n",
    "        price_text = price.text[8:].strip() if price else None\n",
    "        w_v_text = w_v.text.strip() if w_v else None\n",
    "\n",
    "        # Construct category path\n",
    "        category = \" -> \".join([elem.text for elem in category_elements]) if category_elements else None\n",
    "\n",
    "        # Determine count, quantity, and unit from weight/volume text\n",
    "        count = 1\n",
    "        unit = \"\"\n",
    "        if not w_v_text and title_text:\n",
    "            sample = title_text.split()\n",
    "            units = [\"kg\", \"ml\", \"pc\"]\n",
    "            prev = \"\"\n",
    "            for unit in units:\n",
    "                for word in sample:\n",
    "                    if unit in word:\n",
    "                        w_v_text = prev + word\n",
    "                        unit = word.strip()\n",
    "                        break\n",
    "                    prev = word\n",
    "        \n",
    "        if w_v_text:\n",
    "            w_v_text = w_v_text.strip()\n",
    "            unit = w_v_text[-2:].strip()\n",
    "            if \"x\" in w_v_text:\n",
    "                w_v_parts = w_v_text.split(\"x\")\n",
    "                count = w_v_parts[0].strip()\n",
    "                if \"pc\" in w_v_parts[1]:\n",
    "                    w_v_text = w_v_parts[1][:-3].strip()\n",
    "                    unit = \"pcs\"\n",
    "                else:\n",
    "                    numbers = re.findall(r'\\d+', w_v_parts[1])\n",
    "                    alphabets = re.findall(r'[a-zA-Z]+', w_v_parts[1])\n",
    "                    w_v_text = ' '.join(numbers)\n",
    "                    unit = ' '.join(alphabets)\n",
    "            else:\n",
    "                if \" \" in w_v_text:\n",
    "                    parts = w_v_text.split()\n",
    "                    w_v_text = parts[0]\n",
    "                    unit = parts[1]\n",
    "                else:\n",
    "                    numbers = re.findall(r'\\d+', w_v_text)\n",
    "                    alphabets = re.findall(r'[a-zA-Z]+', w_v_text)\n",
    "                    w_v_text = ' '.join(numbers)\n",
    "                    unit = ' '.join(alphabets)\n",
    "\n",
    "        # Extract image links\n",
    "        image_links = set()\n",
    "        image_divs = soup.find_all('div',class_=\"relative h-full w-full\")\n",
    "        # print(images)\n",
    "        for div in image_divs:\n",
    "            img = div.find(\"img\", lazyboundary=\"800px\")\n",
    "            if img:\n",
    "                link = img.get('src')\n",
    "                if link:\n",
    "                    if not link.startswith('data:image/gif;'):\n",
    "                        link = link.split(\"?\")[0]\n",
    "                        link = link.replace(\"/s/\", \"/l/\")\n",
    "                        image_links.add(link)\n",
    "\n",
    "        # Extract \"About the Product\" and \"Other Product Info\" sections\n",
    "        details = {\"About the Product\":None,\"Other Product Info\":None,\"EAN Code\":None}\n",
    "        about_section = soup.find('span', string='About the Product')\n",
    "        other_info_section = soup.find('span', string='Other Product Info')\n",
    "\n",
    "        if about_section:\n",
    "            about_content = about_section.find_next(\"div\").find_next(\"div\")\n",
    "            details[\"About the Product\"] = about_content.text.strip() if about_content else None\n",
    "\n",
    "        if other_info_section:\n",
    "            other_info_content = other_info_section.find_next(\"div\").find_next(\"div\")\n",
    "            details[\"Other Product Info\"] = other_info_content.text.strip() if other_info_content else None\n",
    "            if \"EAN\" in details.get(\"Other Product Info\", \"\"):\n",
    "                ean_matches = re.findall(r'\\d+', details[\"Other Product Info\"])\n",
    "                details[\"EAN Code\"] = ean_matches[0] if ean_matches else None\n",
    "\n",
    "        # Construct the product dictionary\n",
    "        product = {\n",
    "            \"Brand\": brand_text,\n",
    "            \"Title\": title_text,\n",
    "            \"Category\": category,\n",
    "            \"Price\": price_text,\n",
    "            \"Count\": count,\n",
    "            \"Quantity\": w_v_text,\n",
    "            \"Unit\": unit,\n",
    "            \"Image Link\": list(image_links),\n",
    "            \"Url\":url_page\n",
    "        }\n",
    "\n",
    "        product.update(details)\n",
    "        return product\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e} while processing {url_page}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Function to handle multithreading\n",
    "def scrap_product_concurrently(product_links):\n",
    "    product_details = []\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:  # Adjust max_workers as needed\n",
    "        future_to_url = {executor.submit(scrap_product, url): url for url in product_links}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                product = future.result()\n",
    "                if product:\n",
    "                    product_details.append(product)\n",
    "            except Exception as exc:\n",
    "                print(f'{url} generated an exception: {exc}')\n",
    "    return product_details\n",
    "\n",
    "# Main function\n",
    "if __name__ == '__main__':\n",
    "    category = search_category()\n",
    "    print(category)\n",
    "    product_detail = []\n",
    "    if category:\n",
    "        product_links = scrap_category(category)\n",
    "        print(f\"Found {len(product_links)} products.\")\n",
    "        if product_links:\n",
    "            product_detail = scrap_product_concurrently(product_links)\n",
    "    else:\n",
    "        print(\"No category found\")\n",
    "\n",
    "    if product_detail:\n",
    "        keys = product_detail[0].keys()\n",
    "        with open('bigbasket_data.csv', 'a', newline='', encoding='utf-8') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "            # dict_writer.writeheader()  # Uncomment if writing the header for the first time\n",
    "            dict_writer.writerows(product_detail)\n",
    "    else:\n",
    "        print(\"No product found\")\n",
    "\n",
    "    print(\"Processing done\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c62d67cd-50fd-44a7-954d-7b43c28e6cb6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
